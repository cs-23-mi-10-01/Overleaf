\subsection{Hypothesis on Joint Timestamp Selection}
%Joined Timestamp Selection
%Cooperative Timestamp Selection
%Timestamp Agreement
%Continuous Timestamp Value Agreement
%Continuous Timestamp Value Selection
%Continuous Timestamp Value Averaging
%Timestamp Prediction Average
\label{sec:hypothesis_timestamp_voting}

We observed that timestamps are a continuous value and therefore has varying degrees of error which can be measured using \gls{mae}. 
As such it is possible to find the average between a number of different timestamps enabling us to combine the answers of models, where the result is an average of the answers.
From this we have created the hypothesis:

\begin{hypothesis}
\label{hyp:timestamp_voting}
Time predictions produced by taking the mean average of all models have a lower \gls{mae} than time predictions produced by each individual model.
\end{hypothesis}

To evaluate the models individually, as well as to compare them with the average results of all models, we use \gls{mae} between the predicted timestamp and the correct timestamp, over all time prediction tasks. It is defined as

\begin{equation}
\mathit{MAE}(T, m) = \frac{1}{|T|} \varsum_{f \in T} | \timebegin{p_\tau} - \timebegin{f_\tau} |
\end{equation}

\noindent
where $T$ is a test set containing only prediction tasks targeting the first timestamp, $m$ is a model, $p_\tau$ is the timespan of the predicted fact of model $m$ when evaluating $f$, and $f_\tau$ is the timespan of $f$.

We evaluate the \gls{mae} of the individual models, to see how precise they are at making time predictions. To our knowledge, this way of evaluating time predictions has no precedent in other papers. As such, we will instead evaluate whether the results are within an acceptable range for potential use in \gls{qa} systems.

If the \gls{mae} of the joint timestamp results is lower than the \gls{mae} of each individual model, the hypothesis is deemed true.
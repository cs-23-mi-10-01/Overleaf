\subsection{Temporal Precision}
\label{subsec:temporal_precision}

% \begin{hypothesis}
% \label{hyp:temporal_precision}
% Prediction quality of queries where the prediction target is a timestamp is improved by evaluating on precision rather than rank.
% \end{hypothesis}

This experiment is inspired by prior work \cite{P9}, which showed that predictions on timestamps had considerably lower quality than predictions on other fact elements.
However, in contrast to the remaining fact elements, timestamps are a continuous rather than discrete value, and as such it is possible to measure degrees of precision.

\input{content/preliminary_experiments/tables/temporal_precision}

The issue is exemplified in \autoref{tab:temporal_precision}, which shows hypothetical results from two models on the same query.
In this example both models rank the correct answer second, therefore obtaining the same score.
However, if we look at the other predictions made by both models \textsc{MODEL1} is fairly close, while \textsc{MODEL2} is way off.
Therefore the purpose of this experiment is to examine how close the  predictions on timestamps are to the correct results, instead of evaluating how far down the list of proposed results the correct one is.

This is done with two different approaches:
\begin{enumerate}
    \item Calculating MRR with different degrees of precision.
    \item Calculating distance between best rank and correct result.
\end{enumerate}

\subsubsection{The first approach} changes the accepted degree of precision such that predictions within a certain margin are accepted as correct too. In this case a query evaluated with a precision of 1 time step would have three accepted answers: The time step before, the exact time step and the time step after. The set of accepted predictions is defined as 

\begin{equation}
\begin{gathered}
A_m = \{ p_i \mid \tau_{q} - m \le \tau_{p_i} \le \tau_{q} + m \},\\
m \in \mathbb{N}
\end{gathered}
\end{equation}

where $m$ is the margin. 

The score of a model is \gls{mrr} calculated based on the highest ranked predictions within the margin, with 1 being the best possible score.
When the models from \autoref{tab:temporal_precision} are scored with this approach and a margin of $m=1$, \textsc{MODEL1} gets a rank of 1 and \textsc{MODEL2} gets a rank of 2.

\subsubsection{The second approach}
considers the mean absolute error between the beginning timestamp of the best ranked prediction and the correct result, over all timestamp prediction tasks. It is evaluated using the function $\mathit{diff}$.

\begin{equation}
\mathit{diff}(T, m) = \frac{\varsum_{(h, r, t, \tau) \in T} | p_\tau - \tau | }{|T|}
\end{equation}

\noindent
where $T$ is a testset containing only timestamp prediction tasks, $m$ is a model, and $p_\tau$ is the timestamp of the predicted fact of model $m$ when evaluating $(h, r, t, \tau)$.

The score of a model is the average distance over all queries, with 0 being the best possible score. When the models from \autoref{tab:temporal_precision} are scored with this approach \textsc{MODEL1} gets a score of 1 time step and \textsc{MODEL2} gets a score of 11.595 time steps.

\input{content/preliminary_experiments/tables/temporal_precision_result}
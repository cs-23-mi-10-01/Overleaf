\subsection{Temporal Precision}
\label{subsec:temporal_precision}

% \begin{hypothesis}
% \label{hyp:temporal_precision}
% Prediction quality of queries where the prediction target is a timestamp is improved by evaluating on precision rather than rank.
% \end{hypothesis}

This experiment is inspired by prior work \cite{P9}, which showed that predictions on timestamps had considerably lower quality than predictions on other fact elements.
However, in contrast to the remaining fact elements, timestamps are a continuous rather than discrete value, and as such it is possible to measure degrees of precision.

\input{content/preliminary_experiments/tables/temporal_precision}

The issue is exemplified in \autoref{tab:temporal_precision}, which shows hypothetical results from two models on the same query.
In this example both models rank the correct answer second, therefore obtaining the same score.
However, if we look at the other predictions made by both models \textsc{MODEL1} is fairly close, while \textsc{MODEL2} is way off.
Therefore the purpose of this experiment is to examine how close the  predictions on timestamps are to the correct results, instead of evaluating how far down the list of proposed results the correct one is.

This is done with two different approaches:
\begin{enumerate}
    \item Calculating MRR with different degrees of precision.
    \item Calculating distance between best rank and correct result.
\end{enumerate}

\subsubsection{The first approach} changes the accepted degree of precision such that predictions within a certain margin are accepted as correct too. In this case a query evaluated with a precision of 1 time step would have three accepted answers: The time step before, the exact time step and the time step after. The set of accepted predictions is defined as 

\begin{equation}
\begin{gathered}
A_m = \{ p_i \mid \tau_{q} - m \le \tau_{p_i} \le \tau_{q} + m \},\\
m \in \mathbb{N}
\end{gathered}
\end{equation}

where $m$ is the margin. 

The score of a model is \gls{mrr} calculated based on the highest ranked predictions within the margin, with 1 being the best possible score.
When the models from \autoref{tab:temporal_precision} are scored with this approach and a margin of $m=1$, \textsc{MODEL1} gets a rank of 1 and \textsc{MODEL2} gets a rank of 2.

\subsubsection{The second approach}
considers only the difference between the best ranked prediction and the correct result. This difference is defined with the function $\mathit{diff}$, which finds the difference between a predicted fact $p$ and a true fact $f$, and maps it to an natural number $(\varE, \varR, \varE, \varT^2) \times (\varE, \varR, \varE, \varT^2) \rightarrow \N$. The timestep difference between two facts $p$ and $t$ is defined as being equal to the number of granularity timespans where the beginning timestamp of either $p$ or $t$ falls in or between them. It is defined as

\begin{equation}
\begin{gathered}
\mathit{diff}(p, t) = |D|\\
D = \{ g_i \in \varT^2 \mid
\begin{aligned}
&(\timebegin{g_i} \le \timebegin{\tau_p} \vee \timebegin{g_i} \le \timebegin{\tau_t}) \, \wedge \\
&(\timebegin{\tau_p} < \timeend{g_i} \vee \timebegin{\tau_t} < \timeend{g_i})
\end{aligned}
\}
\end{gathered}
\end{equation}

\noindent
where $g_i$ are granularity timespans as defined in section \missing[X], $\tau_{p}$ is the timespan of fact $p$, and $\tau_t$ is the timespan of fact $t$. Note that the facts will always fall into at least one granularity timespan, and therefore the timestamp difference between two facts is always at least 1.

The score of a model is the average distance over all queries, with 1 being the best possible score. When the models from \autoref{tab:temporal_precision} are scored with this approach \textsc{MODEL1} gets a score of 1 time step and \textsc{MODEL2} gets a score of 11.595 time steps.

\input{content/preliminary_experiments/tables/temporal_precision_result}
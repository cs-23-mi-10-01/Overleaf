\subsection{Temporal Precision}
\label{subsec:temporal_precision}

% \begin{hypothesis}
% \label{hyp:temporal_precision}
% Prediction quality of queries where the prediction target is a timestamp is improved by evaluating on precision rather than rank.
% \end{hypothesis}

This experiment is inspired by prior work \cite{P9}, which showed that predictions on timestamps had considerably lower quality than predictions on other fact elements.
However, in contrast to the remaining fact elements, timestamps are a continuous rather than discrete value, and as such it is possible to measure degrees of precision.

\input{content/preliminary_experiments/tables/temporal_precision}

The issue is exemplified in \autoref{tab:temporal_precision}, which shows hypothetical results from two models on the same query.
In this example both models rank the correct answer second, therefore obtaining the same score.
However, if we look at the other predictions made by both models \textsc{MODEL1} is fairly close, while \textsc{MODEL2} is way off.
Therefore the purpose of this experiment is to examine how close the  predictions on timestamps are to the correct results, instead of evaluating how far down the list of proposed results the correct one is.

This is done with two different approaches:
\begin{enumerate}
    \item Calculating MRR with different degrees of precision.
    \item Calculating distance between best rank and correct result.
\end{enumerate}

\subsubsection{The first approach} changes the accepted degree of precision such that predictions within a certain margin are accepted as correct too. In this case a query evaluated with a precision of 1 time step would have three accepted answers: The time step before, the exact time step and the time step after. The set of accepted predictions is defined as 

\begin{equation}
\begin{gathered}
A_m = \{ p_i \mid \tau_{q} - m \le \tau_{p_i} \le \tau_{q} + m \},\\
m \in \mathbb{N}
\end{gathered}
\end{equation}

where $m$ is the margin. 

The score of a model is \gls{mrr} calculated based on the highest ranked predictions within the margin, with 1 being the best possible score.
When the models from \autoref{tab:temporal_precision} are scored with this approach and a margin of $m=1$, \textsc{MODEL1} gets a rank of 1 and \textsc{MODEL2} gets a rank of 2.

\subsubsection{The second approach}
considers only the distance between the best ranked prediction, $p_1$, and the correct result, $q$. The distance is the number of time steps between the two timestamps and defined as

\begin{equation}
\begin{gathered}
\mathit{diff}(p_1, q) = |D|,\\
D = \{ \tau_i \in \varT \mid \tau_{p_1} \le \tau_i < \tau_q \vee \tau_q < \tau_i \le \tau_{p_1} \}
\end{gathered}
\end{equation}

\noindent
where $\tau_{p_1}$

The score of a model is the average distance over all queries, with 0 being the best possible score. When the models from \autoref{tab:temporal_precision} are scored with this approach \textsc{MODEL1} gets a score of 1 time step and \textsc{MODEL2} gets a score of 11.595 time steps.

%If a model scores significantly higher when evaluated with the second approach than with the traditional approach, then the hypothesis holds for that model. The hypothesis holds if it holds for all individual models. The first approach cannot be used to evaluate the hypothesis as the measurement unit is different. It is instead used to compare models against one another.